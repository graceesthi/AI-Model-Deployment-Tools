{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0XJK8zdY322"
   },
   "source": [
    "# Yolo Deployment\n",
    "Welcome to this project where we will be exploring the exciting world of object detection using YOLO.\n",
    "\n",
    "Our main objective in this project is to deploy a huge model like Yolo within a specific environement different from its training one.\n",
    "\n",
    "This project involves the following steps:\n",
    "\n",
    "\n",
    "* Cloning YOLO from the main repository\n",
    "* Installing the necessary requirements\n",
    "* Preparing the appropriate versions for optimal performance\n",
    "* Bringing in the weights files (.pt)\n",
    "* Converting the weights files to **ONNX** format (Open Neural Network Exchange)\n",
    "\n",
    "By following these steps, you will gain a deeper understanding of deployment a DL model into a target (edge), as well as the practical skills needed to implement it in your own projects. So let's dive into the fascinating world of object detection with a deployed YOLO !\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3ENiitudDBb"
   },
   "source": [
    "## Have the model trained and validated\n",
    "Clone YOLO from the main repository, install the necessary requirements. Check the download repo in the files explorer in this environment (left side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xw4aNnUmki6B",
    "outputId": "43b90364-2d08-4f90-da41-b751db7a0114"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/ultralytics/YOLOv5\n",
    "# %cd YOLOv5/ # Install dependencies.\n",
    "# !pip install -r requirements.txt\n",
    "# #!pip install onnx\n",
    "# #%cd models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiE8099ZBWK-"
   },
   "source": [
    "# Install stuff to run Yolo\n",
    "It installs all the necessary dependencies for the YOLOv5 model to run. Dependencies are additional software packages that are required for a program to function properly. In this case, the YOLOv5 model requires certain Python packages to be installed in order to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pLjKcLf8xE30",
    "outputId": "3eed8d08-f3fd-49ef-d0a7-0d0d90419842"
   },
   "outputs": [],
   "source": [
    "# !cd YOLOv5 && pip install -r requirements.txt # Install dependencies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ShtlgQ-RsvBp"
   },
   "source": [
    "## Install packages\n",
    "To convert model tp onnx format, we need to install the onnx and the compatible torch version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "j9RcrvX8822K",
    "outputId": "a2817d3e-e649-41a0-9e35-6d16a7d8a361"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch==1.11.0\n",
      "  Downloading torch-1.11.0-cp39-cp39-win_amd64.whl.metadata (24 kB)\n",
      "Collecting torchvision==0.12.0\n",
      "  Downloading torchvision-0.12.0-cp39-cp39-win_amd64.whl.metadata (10 kB)\n",
      "Collecting torchaudio==0.11.0\n",
      "  Downloading torchaudio-0.11.0-cp39-cp39-win_amd64.whl.metadata (1.0 kB)\n",
      "Collecting torchtext==0.12.0\n",
      "  Downloading torchtext-0.12.0-cp39-cp39-win_amd64.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\thioy\\appdata\\roaming\\python\\python39\\site-packages (from torch==1.11.0) (4.10.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\thioy\\appdata\\roaming\\python\\python39\\site-packages (from torchvision==0.12.0) (1.24.4)\n",
      "Requirement already satisfied: requests in c:\\users\\thioy\\appdata\\roaming\\python\\python39\\site-packages (from torchvision==0.12.0) (2.32.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\thioy\\appdata\\roaming\\python\\python39\\site-packages (from torchvision==0.12.0) (11.0.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\thioy\\appdata\\roaming\\python\\python39\\site-packages (from torchtext==0.12.0) (4.66.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision==0.12.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\thioy\\appdata\\roaming\\python\\python39\\site-packages (from requests->torchvision==0.12.0) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision==0.12.0) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision==0.12.0) (2022.12.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\thioy\\appdata\\roaming\\python\\python39\\site-packages (from tqdm->torchtext==0.12.0) (0.4.6)\n",
      "Downloading torch-1.11.0-cp39-cp39-win_amd64.whl (157.9 MB)\n",
      "   ---------------------------------------- 157.9/157.9 MB 7.3 MB/s eta 0:00:00\n",
      "Downloading torchvision-0.12.0-cp39-cp39-win_amd64.whl (1.0 MB)\n",
      "   ---------------------------------------- 1.0/1.0 MB 47.2 MB/s eta 0:00:00\n",
      "Downloading torchaudio-0.11.0-cp39-cp39-win_amd64.whl (372 kB)\n",
      "Downloading torchtext-0.12.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 1.5/1.5 MB 15.6 MB/s eta 0:00:00\n",
      "Installing collected packages: torch, torchvision, torchtext, torchaudio\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.5.1\n",
      "    Uninstalling torch-2.5.1:\n",
      "      Successfully uninstalled torch-2.5.1\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.20.1\n",
      "    Uninstalling torchvision-0.20.1:\n",
      "      Successfully uninstalled torchvision-0.20.1\n",
      "Successfully installed torch-1.11.0 torchaudio-0.11.0 torchtext-0.12.0 torchvision-0.12.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting onnx==1.12.0\n",
      "  Downloading onnx-1.12.0-cp39-cp39-win_amd64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\thioy\\appdata\\roaming\\python\\python39\\site-packages (from onnx==1.12.0) (1.24.4)\n",
      "Collecting protobuf<=3.20.1,>=3.12.2 (from onnx==1.12.0)\n",
      "  Downloading protobuf-3.20.1-cp39-cp39-win_amd64.whl.metadata (699 bytes)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in c:\\users\\thioy\\appdata\\roaming\\python\\python39\\site-packages (from onnx==1.12.0) (4.10.0)\n",
      "Downloading onnx-1.12.0-cp39-cp39-win_amd64.whl (11.5 MB)\n",
      "   ---------------------------------------- 11.5/11.5 MB 11.4 MB/s eta 0:00:00\n",
      "Downloading protobuf-3.20.1-cp39-cp39-win_amd64.whl (904 kB)\n",
      "   ---------------------------------------- 904.1/904.1 kB 8.1 MB/s eta 0:00:00\n",
      "Installing collected packages: protobuf, onnx\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.21.9\n",
      "    Uninstalling protobuf-4.21.9:\n",
      "      Successfully uninstalled protobuf-4.21.9\n",
      "Successfully installed onnx-1.12.0 protobuf-3.20.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\thioy\\AppData\\Roaming\\Python\\Python39\\site-packages\\google\\~upb'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "googleapis-common-protos 1.65.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
      "mlflow 2.0.1 requires importlib-metadata!=4.7.0,<6,>=3.7.0, but you have importlib-metadata 6.7.0 which is incompatible.\n",
      "mlflow 2.0.1 requires packaging<22, but you have packaging 24.1 which is incompatible.\n",
      "mysql-connector-python 8.2.0 requires protobuf<=4.21.12,>=4.21.1, but you have protobuf 3.20.1 which is incompatible.\n",
      "tensorflow-intel 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.4 which is incompatible.\n",
      "tensorflow-intel 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 3.20.1 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torch\n",
      "Version: 1.11.0\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3\n",
      "Location: c:\\users\\thioy\\appdata\\roaming\\python\\python39\\site-packages\n",
      "Requires: typing-extensions\n",
      "Required-by: accelerate, bitsandbytes, sentence-transformers, thop, torchaudio, torchtext, torchvision, ultralytics, ultralytics-thop\n",
      "Name: torchvision\n",
      "Version: 0.12.0\n",
      "Summary: image and video datasets and models for torch deep learning\n",
      "Home-page: https://github.com/pytorch/vision\n",
      "Author: PyTorch Core Team\n",
      "Author-email: soumith@pytorch.org\n",
      "License: BSD\n",
      "Location: c:\\users\\thioy\\appdata\\roaming\\python\\python39\\site-packages\n",
      "Requires: numpy, pillow, requests, torch, typing-extensions\n",
      "Required-by: ultralytics\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.11.0 torchvision==0.12.0 torchaudio==0.11.0 torchtext==0.12.0\n",
    "!pip install onnx==1.12.0\n",
    "!pip show torch\n",
    "!pip show torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thioy\\Desktop\\4eme année\\deployment and maintenance of ai models\\TP ONNX\\YOLOv5\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "266fuRnPlQIK",
    "outputId": "1f66f0fe-86bc-42d4-f4d9-01b0cf2f6a3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thioy\\Desktop\\4eme année\\deployment and maintenance of ai models\\TP ONNX\\YOLOv5\\models\n",
      " Le volume dans le lecteur C s'appelle Windows\n",
      " Le num�ro de s�rie du volume est 2455-5271\n",
      "\n",
      " R�pertoire de c:\\Users\\thioy\\Desktop\\4eme ann�e\\deployment and maintenance of ai models\\TP ONNX\\YOLOv5\\models\n",
      "\n",
      "30/10/2024  17:22    <DIR>          .\n",
      "30/10/2024  17:22    <DIR>          ..\n",
      "30/10/2024  17:22                 0 __init__.py\n",
      "30/10/2024  17:22            53�218 common.py\n",
      "30/10/2024  17:22             5�283 experimental.py\n",
      "30/10/2024  17:22    <DIR>          hub\n",
      "30/10/2024  17:22    <DIR>          segment\n",
      "30/10/2024  17:22            34�561 tf.py\n",
      "30/10/2024  17:22            21�525 yolo.py\n",
      "30/10/2024  17:22             1�467 yolov5l.yaml\n",
      "30/10/2024  17:22             1�469 yolov5m.yaml\n",
      "30/10/2024  17:22             1�469 yolov5n.yaml\n",
      "30/10/2024  17:22             1�469 yolov5s.yaml\n",
      "30/10/2024  17:22             1�469 yolov5x.yaml\n",
      "              10 fichier(s)          121�930 octets\n",
      "               4 R�p(s)  97�102�180�352 octets libres\n"
     ]
    }
   ],
   "source": [
    "%cd models\n",
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XdrzjZ5TtN9h"
   },
   "source": [
    "# Download the model .pt (weights) of Yolo\n",
    " We will also bring the weights files (.pt) and then proceed to convert them to ONNX format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TQ-Y_uxjk4im",
    "outputId": "504b0e7a-52c7-41c5-f21a-28d3256161fd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "  8 14.1M    8 1180k    0     0   732k      0  0:00:19  0:00:01  0:00:18  732k\n",
      "100 14.1M  100 14.1M    0     0  6412k      0  0:00:02  0:00:02 --:--:-- 20.2M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Le volume dans le lecteur C s'appelle Windows\n",
      " Le num�ro de s�rie du volume est 2455-5271\n",
      "\n",
      " R�pertoire de c:\\Users\\thioy\\Desktop\\4eme ann�e\\deployment and maintenance of ai models\\TP ONNX\\YOLOv5\\models\n",
      "\n",
      "30/10/2024  17:44    <DIR>          .\n",
      "30/10/2024  17:22    <DIR>          ..\n",
      "30/10/2024  17:22                 0 __init__.py\n",
      "30/10/2024  17:22            53�218 common.py\n",
      "30/10/2024  17:22             5�283 experimental.py\n",
      "30/10/2024  17:22    <DIR>          hub\n",
      "30/10/2024  17:22    <DIR>          segment\n",
      "30/10/2024  17:22            34�561 tf.py\n",
      "30/10/2024  17:22            21�525 yolo.py\n",
      "30/10/2024  17:22             1�467 yolov5l.yaml\n",
      "30/10/2024  17:22             1�469 yolov5m.yaml\n",
      "30/10/2024  17:22             1�469 yolov5n.yaml\n",
      "30/10/2024  17:44        14�808�437 YOLOv5s.pt\n",
      "30/10/2024  17:22             1�469 yolov5s.yaml\n",
      "30/10/2024  17:22             1�469 yolov5x.yaml\n",
      "              11 fichier(s)       14�930�367 octets\n",
      "               4 R�p(s)  96�000�786�432 octets libres\n",
      "[WinError 2] Le fichier spécifié est introuvable: '.. # Export to ONNX.'\n",
      "c:\\Users\\thioy\\Desktop\\4eme année\\deployment and maintenance of ai models\\TP ONNX\\YOLOv5\\models\n"
     ]
    }
   ],
   "source": [
    "# Download .pt model.\n",
    "!curl -L -o YOLOv5s.pt https://github.com/ultralytics/YOLOv5/releases/download/v6.1/YOLOv5s.pt\n",
    "\n",
    "%ls\n",
    "%cd .. # Export to ONNX.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2EAteF7g7nYF",
    "outputId": "a40a61c4-fb70-4b18-e7a1-e134377370c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thioy\\Desktop\\4eme année\\deployment and maintenance of ai models\\TP ONNX\n",
      " Le volume dans le lecteur C s'appelle Windows\n",
      " Le num�ro de s�rie du volume est 2455-5271\n",
      "\n",
      " R�pertoire de c:\\Users\\thioy\\Desktop\\4eme ann�e\\deployment and maintenance of ai models\\TP ONNX\n",
      "\n",
      "30/10/2024  17:22    <DIR>          .\n",
      "30/10/2024  17:18    <DIR>          ..\n",
      "30/10/2024  17:45            31�633 Deployment_of_Yolo5_ONNX.ipynb\n",
      "30/10/2024  17:22    <DIR>          YOLOv5\n",
      "               1 fichier(s)           31�633 octets\n",
      "               3 R�p(s)  95�978�901�504 octets libres\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "%cd YOLOv5\n",
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jkhyqgz2tcJe"
   },
   "source": [
    "## Convert into ONNX\n",
    "\n",
    "ONNX stands for Open Neural Network Exchange. It is an open-source format for representing deep learning models that was developed by Microsoft and Facebook in 2017. ONNX is designed to enable interoperability between different deep learning frameworks, allowing models to be developed in one framework and then used in another.\n",
    "\n",
    "The idea behind ONNX is to create a common format that can be used across different frameworks, making it easier to move models between different environments. This can be particularly useful for developers who want to take advantage of the strengths of different frameworks, or who need to move models between different platforms.\n",
    "\n",
    "ONNX supports a wide range of deep learning models, including convolutional neural networks (CNNs), recurrent neural networks (RNNs), and long short-term memory (LSTM) networks. It also supports a variety of data types, including float, integer, and boolean.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xnwsCjKVmSs0",
    "outputId": "3671b36e-488b-455a-c04c-3f1b1510f8db"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mexport: \u001b[0mdata=C:\\Users\\thioy\\Desktop\\4eme anne\\deployment and maintenance of ai models\\TP ONNX\\YOLOv5\\data\\coco128.yaml, weights=['models/YOLOv5s.pt'], imgsz=[640, 640], batch_size=1, device=cpu, half=False, inplace=False, keras=False, optimize=False, int8=False, per_tensor=False, dynamic=False, simplify=False, mlmodel=False, opset=12, verbose=False, workspace=4, nms=False, agnostic_nms=False, topk_per_class=100, topk_all=100, iou_thres=0.45, conf_thres=0.25, include=['onnx']\n",
      "fatal: cannot change to 'C:\\Users\\thioy\\Desktop\\4eme': No such file or directory\n",
      "YOLOv5  2024-10-30 Python-3.9.13 torch-1.11.0+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from models\\YOLOv5s.pt with output shape (1, 25200, 85) (14.1 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.12.0...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  2.6s, saved as models\\YOLOv5s.onnx (28.0 MB)\n",
      "\n",
      "Export complete (3.4s)\n",
      "Results saved to \u001b[1mC:\\Users\\thioy\\Desktop\\4eme anne\\deployment and maintenance of ai models\\TP ONNX\\YOLOv5\\models\u001b[0m\n",
      "Detect:          python detect.py --weights models\\YOLOv5s.onnx \n",
      "Validate:        python val.py --weights models\\YOLOv5s.onnx \n",
      "PyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', 'models\\YOLOv5s.onnx')  \n",
      "Visualize:       https://netron.app\n"
     ]
    }
   ],
   "source": [
    "!python export.py --weights models/YOLOv5s.pt --include onnx --opset 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93PTy7tCmnyA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting backports.tarfile\n",
      "  Downloading backports.tarfile-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Downloading backports.tarfile-1.2.0-py3-none-any.whl (30 kB)\n",
      "Installing collected packages: backports.tarfile\n",
      "Successfully installed backports.tarfile-1.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install backports.tarfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SAtpCnttE43G"
   },
   "source": [
    "# Download ONNX file\n",
    "Here we download it to deploy it in other environments  (C++, Matlab, python,...etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "HZ7lE3k3mFHG",
    "outputId": "667208b8-9316-4872-ab79-6192c233cdb0"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "download(\"download_762fe5f8-67eb-46fd-ab05-864fd1804ee2\", \"YOLOv5s.onnx\", 29337155)",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Download the file.\n",
    "# from google.colab import files\n",
    "# files.download('models/YOLOv5s.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4kaDB0JFM2S"
   },
   "source": [
    "# Classes names before deployment\n",
    "\n",
    "Yolo inference needs the classes of ground truth. In coco format, you may find it in Yolo githu, or any project using it. https://github.com/mikbuch/yolo-coco-opencv-example/blob/main/coco.names\n",
    "\n",
    "Your task is to download it to be used in inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "FsJDIkgF4CT4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100   621  100   621    0     0   4644      0 --:--:-- --:--:-- --:--:--  4669\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://raw.githubusercontent.com/mikbuch/yolo-coco-opencv-example/main/coco.names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20kTPBWVFdD2"
   },
   "source": [
    "#Onnx deployment in a real test\n",
    "\n",
    "- **draw_label(im, label, x, y)**: This function takes an image, a label, and the x and y coordinates as input parameters. It is used to draw text onto an image at a specific location.\n",
    "\n",
    "- **pre_process(input_image, net)**: This function takes an input image and a neural network as input parameters. It preprocesses the input image by creating a 4D blob and setting the input to the neural network. It then runs a forward pass to get the output of the output layers.\n",
    "\n",
    "- **post_process(input_image, outputs):** This function takes an input image and the output of the neural network as input parameters. It post-processes the output by iterating through the detections, discarding bad detections, getting the class ID of the detection, performing non-maximum suppression, and drawing the bounding box and label onto the image. It returns the processed image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-1bkMgyWBc9q"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Constants.\n",
    "INPUT_WIDTH = 640\n",
    "INPUT_HEIGHT = 640\n",
    "SCORE_THRESHOLD = 0.5\n",
    "NMS_THRESHOLD = 0.45\n",
    "CONFIDENCE_THRESHOLD = 0.45\n",
    "\n",
    "# Text parameters.\n",
    "FONT_FACE = cv2.FONT_HERSHEY_SIMPLEX\n",
    "FONT_SCALE = 1.5\n",
    "THICKNESS = 2\n",
    "\n",
    "# Colors.\n",
    "BLACK  = (0,0,0)\n",
    "BLUE   = (255,178,50)\n",
    "YELLOW = (0,255,255)\n",
    "\n",
    "def draw_label(im, label, x, y):\n",
    "    \"\"\"Draw text onto image at location.\"\"\"\n",
    "    # Get text size.\n",
    "    text_size = cv2.getTextSize(label, FONT_FACE, FONT_SCALE, THICKNESS)\n",
    "    dim, baseline = text_size[0], text_size[1]\n",
    "    # Use text size to create a BLACK rectangle.\n",
    "    cv2.rectangle(im, (x,y), (x + dim[0], y + dim[1] + baseline), (0,0,0), cv2.FILLED);\n",
    "    # Display text inside the rectangle.\n",
    "    cv2.putText(im, label, (x, y + dim[1]), FONT_FACE, FONT_SCALE, YELLOW, THICKNESS, cv2.LINE_AA)\n",
    "\n",
    "def pre_process(input_image, net):\n",
    "      # Create a 4D blob from a frame.\n",
    "      blob = cv2.dnn.blobFromImage(input_image, 1/255,  (INPUT_WIDTH, INPUT_HEIGHT), [0,0,0], 1, crop=False)\n",
    "\n",
    "      # Sets the input to the network.\n",
    "      net.setInput(blob)\n",
    "\n",
    "      # Run the forward pass to get output of the output layers.\n",
    "      outputs = net.forward(net.getUnconnectedOutLayersNames())\n",
    "      return outputs\n",
    "\n",
    "\n",
    "def post_process(input_image, outputs):\n",
    "      # Lists to hold respective values while unwrapping.\n",
    "      class_ids = []\n",
    "      confidences = []\n",
    "      boxes = []\n",
    "      # Rows.\n",
    "      rows = outputs[0].shape[1]\n",
    "      image_height, image_width = input_image.shape[:2]\n",
    "      # Resizing factor.\n",
    "      x_factor = image_width / INPUT_WIDTH\n",
    "      y_factor =  image_height / INPUT_HEIGHT\n",
    "      # Iterate through detections.\n",
    "      for r in range(rows):\n",
    "            row = outputs[0][0][r]\n",
    "            confidence = row[4]\n",
    "            # Discard bad detections and continue.\n",
    "            if confidence >= CONFIDENCE_THRESHOLD:\n",
    "                  classes_scores = row[5:]\n",
    "                  # Get the index of max class score.\n",
    "                  class_id = np.argmax(classes_scores)\n",
    "                  #  Continue if the class score is above threshold.\n",
    "                  if (classes_scores[class_id] > SCORE_THRESHOLD):\n",
    "                        confidences.append(confidence)\n",
    "                        class_ids.append(class_id)\n",
    "                        cx, cy, w, h = row[0], row[1], row[2], row[3]\n",
    "                        left = int((cx - w/2) * x_factor)\n",
    "                        top = int((cy - h/2) * y_factor)\n",
    "                        width = int(w * x_factor)\n",
    "                        height = int(h * y_factor)\n",
    "                        box = np.array([left, top, width, height])\n",
    "                        boxes.append(box)\n",
    "\n",
    "\n",
    "# What is the role of this line ? NMSBoxes\n",
    "      indices = cv2.dnn.NMSBoxes(boxes, confidences, CONFIDENCE_THRESHOLD, NMS_THRESHOLD)\n",
    "      for i in indices:\n",
    "            box = boxes[i]\n",
    "            left = box[0]\n",
    "            top = box[1]\n",
    "            width = box[2]\n",
    "            height = box[3]\n",
    "            # Draw bounding box.\n",
    "            cv2.rectangle(input_image, (left, top), (left + width, top + height), BLUE, 3*THICKNESS)\n",
    "            # Class label.\n",
    "            label = \"{}:{:.2f}\".format(classes[class_ids[i]], confidences[i])\n",
    "            # Draw label.\n",
    "            draw_label(input_image, label, left, top)\n",
    "      return input_image\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ai1Y4EkcBoST",
    "outputId": "bbc9ac67-3ef8-46c4-dee7-326387f41150"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "benchmarks.py\t data\t     LICENSE\t     README.zh-CN.md   tutorial.ipynb\n",
      "CITATION.cff\t detect.py   models\t     requirements.txt  utils\n",
      "classify\t export.py   pyproject.toml  segment\t       val.py\n",
      "CONTRIBUTING.md  hubconf.py  README.md\t     train.py\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CjeACuD1OppW"
   },
   "source": [
    "#Assignment\n",
    "\n",
    "After understanding the role of previous functions, Write a code (in colab or on your PC) to infer ONNX model to test it on images of your choice, considering the follwoing instructions:  \n",
    "\n",
    "1. **Load class names:**\n",
    "Read the names of the classes from the \"coco.names\" file and store them in a list called classes.\n",
    "2. **Load image:**\n",
    "Load an image file \"test_image.jpg\" using OpenCV's imread() function and store it in a variable called frame.\n",
    "3. **Load model weights:**\n",
    "Provide the path to the pre-trained weights file \"YOLOv5s.onnx\" and load the network using the readNet() function.\n",
    "4. **Process image:**\n",
    "Pass the input image through the network and obtain the detections using a custom function called pre_process().\n",
    "5. **Post-process the output:**\n",
    "Use another custom function called post_process() to draw bounding boxes and labels around the detected objects on a copy of the input image.\n",
    "6.** Measure inference time:**\n",
    "Use the getPerfProfile() function to get the overall time for inference and the timings for each layer in the network.\n",
    "7. **Calculate the inference time** and print it.\n",
    "Add the inference time as a label on the output image using putText() function.\n",
    "\n",
    "8. **Display the output image:**\n",
    "Use Matplotlib's imshow() function to display the image with detected objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "biAaEjN2Bgr4"
   },
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\onnx\\onnx_importer.cpp:274: error: (-5:Bad argument) Can't read ONNX file: C:\\Users\\thioy\\Desktop\\4eme année\\deployment and maintenance of ai models\\TP ONNX\\YOLOv5\\models\\YOLOv5s.onnx in function 'cv::dnn::dnn4_v20230620::ONNXImporter::ONNXImporter'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_image.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m weights_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mthioy\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m4eme année\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdeployment and maintenance of ai models\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTP ONNX\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mYOLOv5\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mYOLOv5s.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 9\u001b[0m net \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     12\u001b[0m outputs \u001b[38;5;241m=\u001b[39m pre_process(frame, net)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.8.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\onnx\\onnx_importer.cpp:274: error: (-5:Bad argument) Can't read ONNX file: C:\\Users\\thioy\\Desktop\\4eme année\\deployment and maintenance of ai models\\TP ONNX\\YOLOv5\\models\\YOLOv5s.onnx in function 'cv::dnn::dnn4_v20230620::ONNXImporter::ONNXImporter'\n"
     ]
    }
   ],
   "source": [
    "# your code\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "with open(r\"C:\\Users\\thioy\\Desktop\\4eme année\\deployment and maintenance of ai models\\TP ONNX\\YOLOv5\\coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "frame = cv2.imread(\"test_image.jpg\")\n",
    "weights_path = r\"C:\\Users\\thioy\\Desktop\\4eme année\\deployment and maintenance of ai models\\TP ONNX\\YOLOv5\\models\\YOLOv5s.onnx\"\n",
    "net = cv2.dnn.readNet(weights_path)\n",
    "\n",
    "start_time = time.time()\n",
    "outputs = pre_process(frame, net)\n",
    "inference_time = time.time() - start_time\n",
    "outputs = pre_process(frame, net)\n",
    "processed_image = post_process(frame, outputs)\n",
    "\n",
    "layer_times = net.getPerfProfile()\n",
    "total_time = layer_times[0] * 1e-6  # Convert from microseconds to seconds\n",
    "\n",
    "# Add inference time as label on the output image\n",
    "cv2.putText(frame, f\"Inference Time: {inference_time:.2f} s\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "# Step 5: Post-process the output to draw bounding boxes and labels\n",
    "output_image = post_process(frame.copy(), outputs)\n",
    "\n",
    "# Step 8: Display the output image using Matplotlib\n",
    "plt.imshow(cv2.cvtColor(output_image, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.title(\"Detected Objects\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
